/*

AIM 	   :Hive Additional Use Cases_2 
DATE	   :10-MAR-2018
PreRequest :hive-1.2.2/ RHEL Operation system and MYSQL.

*/
-----------
Usecase 1: 
-----------
1. Login to Mysql and execute the sql file to load the custpayments.customers table:
	mysql (custpayments)> source /home/hduser/hiveusecase/custpayments_ORIG.sql
	
2. Write sqoop command to import data from custpayments.customers table table with 2 mappers, with enclosed by 
   " (As we have ',' in the data itself we are importing in sqoop using --enclosed-by option into the location /user/hduser/custpayments).
   
   
sqoop import --connect jdbc:mysql://localhost/custpayments --username root --password root \
-table customers \
-m 2 \
--split-by customernumber \
--target-dir /user/hduser/custpayments \
--delete-target-dir \
--enclosed-by '\"';

output:

[hduser@Inceptez ~]$ hadoop fs -cat user/hduser/custpayments/* | head -5
"103","Atelier graphique","Schmitt","Carine ","40.32.2555","54, rue Royale","null","Nantes","null","44000","France","1370","21000.00"
"112","Signal Gift Stores","King","Jean","7025551838","8489 Strong St.","null","Las Vegas","NV","83030","USA","1166","71800.00"
"114","Australian Collectors, Co.","Ferguson","Peter","03 9520 4555","636 St Kilda Road","Level 3","Melbourne","Victoria","3004","Australia","1611","117300.00"
"119","La Rochelle Gifts","Labrune","Janine ","40.67.8555","67, rue des Cinquante Otages","null","Nantes","null","44000","France","1370","118200.00"
"121","Baane Mini Imports","Bergulfsen","Jonas ","07-98 9555","Erling Skakkes gate 78","null","Stavern","null","4110","Norway","1504","81700.00"


NOte:If we specify the Enclosing Character(--enclosed-by ',') then all the data in columns will be enclosed by that character.
	
[hduser@Inceptez ~]$ hadoop fs -cat /user/hduser/customers/* | head -2
18/04/04 00:49:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
,103,,,Atelier graphique,,,Schmitt,,,Carine ,,,40.32.2555,,,54, rue Royale,,,null,,,Nantes,,,null,,,44000,,,France,,,1370,,,21000.00,
,112,,,Signal Gift Stores,,,King,,,Jean,,,7025551838,,,8489 Strong St.,,,null,,,Las Vegas,,,NV,,,83030,,,USA,,,1166,,,71800.00,

3.Create a hive external table and load the sqoop imported data to the hive table called custmaster. 
As we have ',' in the data itself we are using quotedchar option below with the csv serde option as given below as example, 
create the table with all columns.

hive cli (retail):

create external table custmaster_all (customerNumber int,customerName string,contactLastName string,contactFirstName string,phone string,
addressLine1 string,addressLine2 string,city string,state string,postalCode string,country string,salesRepEmployeeNumber int,creditLimit double)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
"separatorChar" = ",",
"quoteChar" = "\"") 
LOCATION '/user/hduser/custpayments/';

output:
hive> select * from custmaster_all limit 5;
OK
103     Atelier graphique       Schmitt Carine  40.32.2555      54, rue Royale  null    Nantes  null    44000   France  1370    21000.00
112     Signal Gift Stores      King    Jean    7025551838      8489 Strong St. null    Las Vegas       NV      83030   USA     1166    71800.00
114     Australian Collectors, Co.      Ferguson        Peter   03 9520 4555    636 St Kilda Road       Level 3 Melbourne       Victoria        3004Australia        1611    117300.00
119     La Rochelle Gifts       Labrune Janine  40.67.8555      67, rue des Cinquante Otages    null    Nantes  null    44000   France  1370    118200.00
121     Baane Mini Imports      Bergulfsen      Jonas   07-98 9555      Erling Skakkes gate 78  null    Stavern null    4110    Norway  1504    81700.00
Time taken: 0.238 seconds, Fetched: 5 row(s)


create table custmaster2 (customerNumber int,customername string,contactlastname string,contactfirstname string)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
"separatorChar" = ",",
"quoteChar" = "\"") 
LOCATION '/user/hduser/custpayments';


hive> select * from custmaster2 limit 5;
OK
103     Atelier graphique       Schmitt Carine
112     Signal Gift Stores      King    Jean
114     Australian Collectors, Co.      Ferguson        Peter
119     La Rochelle Gifts       Labrune Janine
121     Baane Mini Imports      Bergulfsen      Jonas


4. Copy the payments.txt into hdfs location /user/hduser/paymentsdata/ and Create an external table namely 
payments with customernumber, checknumber, paymentdate, amount columns to point the imported payments data.

create external table payments (customerNumber int,checknumber string,paymentdate string,amount double)
row format delimited fields terminated by ','
LOCATION '/user/hduser/paymentsdata/';


5. Create an external table called cust_payments in avro format and load data by 
doing inner join of custmaster and payments tables, using insert select customernumber, 
contactfirstname,contactlastname,phone, creditlimit from custmaster and paymentdate, amount columns from payments table:

create external table cust_payment(custno int,contactfirstname string,contactlastname string,phone string,creditlimit double,paymentdate string,amount double)
stored as avro 
location '/user/hduser/paymentsavro';

insert into table cust_payment select a.customernumber,a.contactfirstname,a.contactlastname,a.phone,a.creditlimit,b.paymentdate,b.amount from 
retail.custmaster a inner join custdb.payments b on a.customernumber = b.customernumber; 

hive> select * from cust_payment limit 5;
custno    contactfirstname   contactlastname    phone      		creditlimit        paymentdate    	amount
103     		Carine  		Schmitt 		40.32.2555      21000.0 			2016-10-19      6066.78
103     		Carine  		Schmitt 		40.32.2555      21000.0 			2016-10-05      14571.44
103     		Carine  		Schmitt 		40.32.2555      21000.0 			2016-10-18      1676.14
112     		Jean    		King    		7025551838      71800.0 			2016-10-17      14191.12
112     		Jean    		King    		7025551838      71800.0 			2016-10-06      32641.98


6.Create a view called custpayments_vw to only display customernumber,creditlimit,paymentdate and amount selected from cust_payments.

CREATE VIEW IF NOT EXISTS custpayments_vw AS select custno,creditlimit,paymentdate  from cust_payment;
--with amount
CREATE VIEW IF NOT EXISTS custpayments_vw2 AS select custno,creditlimit,paymentdate,amount  from cust_payment;

hive> select * from custpayments_vw2;

custno  creditlimit paymentdate     amount
103     21000.0     2016-10-19      6066.78
103     21000.0     2016-10-05      14571.44
103     21000.0     2016-10-18      1676.14
112     71800.0     2016-10-17      14191.12


7. Extract only customernumber,creditlimit,paymentdate and amount columns either using 
the above view/cust_payments table into hdfs location /user/hduser/custpaymentsexport with '|' delimiter.

insert overwrite directory '/user/hduser/custpaymentsexport'
row format delimited fields terminated by '|'
select * from custpayments_vw2;

[hduser@Inceptez ~]$ hadoop fs -cat /user/hduser/custpaymentsexport/* | head -5
103|21000.0|2016-10-19|6066.78
103|21000.0|2016-10-05|14571.44
103|21000.0|2016-10-18|1676.14
112|71800.0|2016-10-17|14191.12
112|71800.0|2016-10-06|32641.98



8.Export the data from the /user/hduser/custpaymentsexport location to mysql table called cust_payments using 
sqoop export with staging table option using records per statement 100 and mappers 3.

In Mysql -> use custdb -> create following tables.
	CREATE TABLE cust_payments (custno INT,creditlimit VARCHAR(100),paymentdate date,amount double);
	CREATE TABLE cust_payments_statging (custno INT,creditlimit VARCHAR(100),paymentdate date,amount double);
	

Export Solution:
sqoop export -Dsqoop.export.records.per.statement=100 --connect jdbc:mysql://localhost/custdb --username root --password root \
--table cust_payments \
-m 3 \
--fields-terminated-by '|' \
--export-dir /user/hduser/custpaymentsexport \
--staging-table cust_payments_statging \
--clear-staging-table \
--columns custno,creditlimit,paymentdate,amount;

18/04/04 11:57:44 INFO mapreduce.ExportJobBase: Transferred 12.7842 KB in 18.1117 seconds (722.7927 bytes/sec)
18/04/04 11:57:44 INFO mapreduce.ExportJobBase: Exported 273 records.
18/04/04 11:57:44 INFO mapreduce.ExportJobBase: Starting to migrate data from staging table to destination.
18/04/04 11:57:44 INFO manager.SqlManager: Migrated 273 records from `cust_payments_statging` to `cust_payments`

verify mysql cust_payments.

select * from cust_payments limit 5;
+--------+-------------+-------------+----------+
| custno | creditlimit | paymentdate | amount   |
+--------+-------------+-------------+----------+
|    333 | 51600.0     | 2016-10-01  | 21432.31 |
|    334 | 98800.0     | 2016-10-27  | 45785.34 |
|    334 | 98800.0     | 2016-10-16  | 29716.86 |
|    334 | 98800.0     | 2016-10-22  | 28394.54 |
|    339 | 81100.0     | 2016-10-24  | 23333.06 |
+--------+-------------+-------------+----------+
5 rows in set (0.00 sec)

-------------------------------------------------------------------------------------------------------------------------------

-----------
Usecase 2: 
-----------

linux location cd /home/hduser/hive/data/cust_fixed_raw


Managing Fixed Width Data:
1. Copy the below fixed data into a linux file, load into a hive table called cust_fixed_raw in a column rawdata.

1  Lara        chennai     55 2016-09-2110000
2  vasudevan   banglore    43 2016-09-2390000
3  Paul        chennai     33 2019-02-2020000
4  David Hanna New Jersey  29 2019-04-22

hive -> use retail;
create table cust_fixed_raw(rawdata varchar(45));
LOAD DATA LOCAL INPATH '/home/hduser/hive/data/cust_fixed_raw' INTO TABLE cust_fixed_raw;

hive> select * from cust_fixed_raw;
1  Lara        chennai     55 2016-09-2110000
2  vasudevan   banglore    43 2016-09-2390000
3  Paul        chennai     33 2019-02-2020000
4  David Hanna New Jersey  29 2019-04-22

2. Create a temporary table called cust_delimited_parsed_temp with columns such as id,name,city,age,dt,amt and load 
the cust_fixed_raw table using substr.for eg to select id column : select trim(substr(rawdata,1,3)) from cust_fixed_raw;

create temporary table cust_delimited_parsed_temp(custid int,name string,city string,age int,dt date,amt double);

insert into table cust_delimited_parsed_temp
select trim(substr(rawdata,1,3)),trim(substr(rawdata,4,12)),trim(substr(rawdata,16,12)),trim(substr(rawdata,28,3)),
trim(substr(rawdata,31,10)),trim(substr(rawdata,41,5)) from cust_fixed_raw;

hive> select * from cust_delimited_parsed_temp;

1       Lara    chennai 55      2016-09-21      10000.0
2       vasudevan       banglore        43      2016-09-23      90000.0
3       Paul    chennai 33      2019-02-20      20000.0
4       David Hanna     New Jersey      29      2019-04-22      NULL


create table tempbkp(custid int,name string,city string,age int,dt date,amt double);
insert into tempbkp select * from cust_delimited_parsed_temp;

3. Export only id, dt and amt column into a mysql table cust_fixed_mysql using sqoop export.
mysql - > use custdb -> create table cust_fixed_mysql(custid int,dt date,amt double);

create external table cust_delimited(custid int,dt date,amt double)
row format delimited fields terminated by ','
location '/user/hduser/cust_delimited';

insert into table cust_delimited select custid,dt,amt from cust_delimited_parsed_temp; 

4,2019-04-22,\N  --input-null-non-string "\\\\N"


sqoop export --connect jdbc:mysql://localhost/custdb --username root --password root \
--table cust_fixed_mysql2 \
--export-dir /user/hduser/cust_delimited \
--columns custid,dt,amt \
--input-null-non-string "\\\\N";


mysql> select * from cust_fixed_mysql;
+--------+------------+-------+
| custid | dt         | amt   |
+--------+------------+-------+
|      4 | 2019-04-22 |  NULL |
|      1 | 2016-09-21 | 10000 |
|      2 | 2016-09-23 | 90000 |
|      3 | 2019-02-20 | 20000 |
+--------+------------+-------+

4. Load only chennai data to another table called cust_parsed_orc of type orc format partitioned based on dt.

static partition (Load):*Its good Approach
------------------------
create table cust_parsed_orc(custid int,name string,age int,dt date,amt double)
partitioned by (city string)
stored as orc;	
	
LOAD DATA INPATH '/user/hive/warehouse/retail.db/tempbkp'
OVERWRITE INTO TABLE cust_parsed_orc PARTITION (city='chennai');

[hduser@Inceptez ~]$ hadoop fs -ls /user/hive/warehouse/retail.db/cust_parsed_orc/*
-rw-r--r--   1 hduser supergroup        159 2020-09-09 13:20 /user/hive/warehouse/retail.db/cust_parsed_orc/city=chennai/000000_0
	
Dynamic:
--------
create table cust_parsed_orc2(custid int,name string,city string,age int,amt double)
partitioned by (dt date)
stored as orc;

Insert into table cust_parsed_orc2 partition (dt)
select custid,name,city,age,amt,dt from tempbkp temp where city='chennai';


[hduser@Inceptez ~]$ hadoop fs -ls /user/hive/warehouse/retail.db/cust_parsed_orc2/*

-rw-r--r--   1 hduser supergroup        560 2020-09-09 16:34 /user/hive/warehouse/retail.db/cust_parsed_orc2/dt=2016-09-21/000000_0
-rw-r--r--   1 hduser supergroup        560 2020-09-09 16:34 /user/hive/warehouse/retail.db/cust_parsed_orc2/dt=2019-02-20/000000_0

mysql -> use metastore:
mysql> select * from PARTITIONS where part_id in (42,43,44);
+---------+-------------+------------------+---------------+-------+--------+----------------+
| PART_ID | CREATE_TIME | LAST_ACCESS_TIME | PART_NAME     | SD_ID | TBL_ID | LINK_TARGET_ID |
+---------+-------------+------------------+---------------+-------+--------+----------------+
|      42 |  1599649499 |                0 | dt=2016-09-21 |   132 |     89 |           NULL |
|      43 |  1599649499 |                0 | dt=2019-02-20 |   133 |     89 |           NULL |
|      44 |  1599651327 |                0 | city=chennai  |   135 |     90 |           NULL |
+---------+-------------+------------------+---------------+-------+--------+----------------+


5. Create a json table called cust_parsed_json (to load into json use the following steps).

cd /home/hduser/hiveusecase
wget https://repo1.maven.org/maven2/org/apache/hive/hcatalog/hive-hcatalog-core/1.2.1/hive-hcatalog-core-1.2.1.jar

hive cli>
add jar /home/hduser/hiveusecase/hive-hcatalog-core-1.2.1.jar;
add jar /home/hduser/hiveusecase/hivexmlserde-1.0.5.3.jar


create external table cust_parsed_json(id int, name string,city string, age int)
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
stored as textfile
location '/user/hduser/custjson';


6. Insert into the cust_parsed_json only non chennai data using insert select of id,name,city, age from the cust_delimited_parsed_temp table.

insert into cust_parsed_json select custid,name,city,age from cust_delimited_parsed_temp temp where city not in ('chennai');

hive> select * from cust_parsed_json;
OK
2       vasudevan       banglore        43
4       David Hanna     New Jersey      29

[hduser@Inceptez hiveusecase]$ hadoop fs -cat /user/hduser/custjson/*
{"id":2,"name":"vasudevan","city":"banglore","age":43}
{"id":4,"name":"David Hanna","city":"New Jersey","age":29}



7. Schema migration:
Convert the XML table called xml_bank created in the actual usecase to JSON data by the same way like step 5 using create table as select.

error:
FAILED: SemanticException [Error 10070]: CREATE-TABLE-AS-SELECT cannot create external table
so created mangaed table.

create table xml_json
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
stored as textfile
location '/user/hduser/custxmljson'
as select * from xml_bank;


hive> select * from xml_json;
OK
0000-JTALA      200000  {"spousedcat":"1","empcat":"2","gender":"F","jobsat":"1","homeown":"0","edcat":"1","hometype":"2","addresscat":"2","marital":"1","jobcat":"2","retire":"0","residecat":"4","agecat":"1"}   {"income":"18","default":"0","creddebt":"1.003392","othdebt":"2.740608"}
0000-KDELL      10000   {"spousedcat":"1","empcat":"3","gender":"M","jobsat":"1","homeown":"0","edcat":"1","hometype":"3","addresscat":"2","marital":"1","jobcat":"2","retire":"1","residecat":"4","agecat":"2"}   {"income":"20","default":"0","creddebt":"1.002292","othdebt":"2.113208"}


8. Import data from mysql directly creating static partition based on city=chennai as given below for additional knowledge.


Must specify destination with --target-dir.

sqoop import --connect jdbc:mysql://localhost:3306/custdb --username root --password root \
--query "select custid,firstname,age from customer where city='chennai' and \$CONDITIONS" \
--target-dir /user/hduser/hiveext/ \
--split-by custid \
--hive-overwrite \
--hive-import \
--create-hive-table \
--hive-partition-key city \
--hive-partition-value 'chennai' \
--fields-terminated-by ',' \
--hive-table default.custinfo \
--direct

18/04/04 15:25:17 INFO hive.HiveImport: Loading data to table default.custinfo partition (city=chennai)
18/04/04 15:25:18 INFO hive.HiveImport: Partition default.custinfo{city=chennai} stats: [numFiles=5, numRows=0, totalSize=70, rawDataSize=0]
18/04/04 15:25:18 INFO hive.HiveImport: OK
18/04/04 15:25:18 INFO hive.HiveImport: Time taken: 0.959 seconds
18/04/04 15:25:18 INFO hive.HiveImport: Hive import complete.
18/04/04 15:25:18 INFO hive.HiveImport: Export directory is contains the _SUCCESS file only, removing the directory.

-------------------------------------------------------------------------------------------------------------------